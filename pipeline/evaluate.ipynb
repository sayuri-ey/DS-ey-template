{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom modules\n",
    "from configs.custom_config import get_timezone\n",
    "from pipeline.train import get_sets\n",
    "from pipeline.train import plot_confusion_matrix\n",
    "from pipeline.train import plot_precision_recall_curve\n",
    "from pipeline.train import plot_roc_curve\n",
    "# external modules\n",
    "import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X_train, y_train):\n",
    "    # Define the hyperparameter space\n",
    "    n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=3)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num=3)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "    param_dist = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'bootstrap': bootstrap\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=1,\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the randomized search cross-validation object to the data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
    "\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "def main():\n",
    "    print(f'Started Evaluate step at {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "    # Load datasets\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Read train, test and valid sets')\n",
    "    X_train, y_train = get_sets('train')\n",
    "    X_test, y_test = get_sets('test')\n",
    "    X_valid, y_valid = get_sets('valid')\n",
    "\n",
    "    # Load model\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Load model')\n",
    "    model = joblib.load('model/model.pkl')\n",
    "\n",
    "    # # Define the hyperparameters to tune Logistic Regression\n",
    "    # param_grid = {\n",
    "    #     'penalty': ['l1', 'l2'],\n",
    "    #     'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    #     'max_iter': [100, 200, 300]\n",
    "    # }\n",
    "    \n",
    "    # # Define the hyperparameters to tune Random Forest\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [100, 200, 300],\n",
    "    #     'max_depth': [5, 10, 15],\n",
    "    #     'min_samples_split': [2, 5, 10],\n",
    "    #     'min_samples_leaf': [1, 2, 4],\n",
    "    #     'max_features': ['sqrt', 'log2']\n",
    "    # }\n",
    "    \n",
    "    # Perform a grid search to find the best hyperparameters\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Tune hyperparameters')\n",
    "    # lr = LogisticRegression()\n",
    "    # rf = RandomForestClassifier()\n",
    "    # estimator = rf\n",
    "    # best_estim = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5)\n",
    "    # best_estim.fit(X_train, y_train)\n",
    "    best_estim = random_search(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Evaluate on test set')\n",
    "    test_score = best_estim.score(X_test, y_test)\n",
    "    print('Test score:', test_score)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Evaluate on valid set')\n",
    "    valid_score = best_estim.score(X_valid, y_valid)\n",
    "    print('Validation score:', valid_score)\n",
    "    \n",
    "    # Metrics for chosen model\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Plotting metrics from test set for the model')\n",
    "    y_pred = best_estim.predict(X_test)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    print(f'Metrics for random_forest: precision: {precision}, recall: {recall}, f1_score: {f1_score}')\n",
    "    plot_precision_recall_curve(y_test, y_pred)\n",
    "    plot_roc_curve(y_test, y_pred)\n",
    "    plot_confusion_matrix(y_test, y_pred)    \n",
    "\n",
    "    # Save the model\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Save tuned model')\n",
    "    joblib.dump(best_estim, 'model/model.pkl')\n",
    "\n",
    "    print(f'Finished Evaluate at {datetime.now().strftime(\"%H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
