{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom modules\n",
    "from configs.custom_config import get_timezone, get_data_path, get_target_column, get_feature_columns, get_categorical_columns, get_numerical_columns\n",
    "# external modules\n",
    "import csv\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment and define functions\n",
    "\n",
    "# set timezone\n",
    "get_timezone()\n",
    "\n",
    "def get_dataset():\n",
    "    dataset = pd.read_csv(get_data_path())\n",
    "    return dataset\n",
    "\n",
    "def split_data(df, test_size=0.50, validation_size=0.25, random_state=42):\n",
    "    \n",
    "    # For random split\n",
    "    target = get_target_column()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), df[target], test_size=test_size, random_state=random_state, stratify=df[target])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=random_state, stratify=y_train)\n",
    "\n",
    "    # Combine the train, validation, and test sets into a single dataframe\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    val = pd.concat([X_val, y_val], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # For temporal split\n",
    "    temporal_column = get_temporal_column()\n",
    "    forecast_df = forecast_df.sort_values(by=temporal_column).reset_index()\n",
    "\n",
    "    train_split = int(test_size * len(forecast_df))\n",
    "    val_split = int((test_size + validation_size) * len(forecast_df))\n",
    "\n",
    "    train_data = forecast_df.iloc[:train_split]\n",
    "    val_data = forecast_df.iloc[train_split:val_split]\n",
    "    test_data = forecast_df.iloc[val_split:]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def redefine_data_types(main_df):\n",
    "    # main_df = main_df.replace(',','.', regex=True)\n",
    "    str_to_num = get_str_to_num()\n",
    "    for feature in str_to_num:\n",
    "        main_df[feature] = pd.to_numeric(main_df[feature], errors='coerce')\n",
    "    num_to_str = get_num_to_str()\n",
    "    for feature in num_to_str:\n",
    "        main_df[feature] = main_df[feature].astype(str)\n",
    "    return main_df\n",
    "\n",
    "def drop_columns(main_df):\n",
    "    cols_to_drop = get_cols_to_drop()    \n",
    "    for col in cols_to_drop:\n",
    "        main_df.drop(col, axis=1, inplace=True)\n",
    "    return main_df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "\n",
    "    # input missing values with mode\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # input missing values with odd value\n",
    "    for col in df.columns:\n",
    "        df[col].fillna(-1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_date_info(df_input):\n",
    "    date_cols = get_date_columns()\n",
    "    df = df_input\n",
    "    list_date_feats = []\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[f\"{col}_day\"] = df[col].dt.day\n",
    "        df[f\"{col}_month\"] = df[col].dt.month\n",
    "        df[f\"{col}_dayofweek\"] = df[col].dt.dayofweek\n",
    "        df[f\"{col}_hour\"] = df[col].dt.hour\n",
    "        df.drop(col, axis=1, inplace=True)  \n",
    "        list_date_feats.append([f\"{col}_day\", f\"{col}_month\", f\"{col}_dayofweek\", f\"{col}_hour\"])\n",
    "    return df, list_date_feats\n",
    "\n",
    "def transform_categorical(df):\n",
    "    cols_to_encode = get_categorical_columns()\n",
    "    le = LabelEncoder()\n",
    "    for col in cols_to_encode:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def scale_numerical(df):\n",
    "    cols_to_scale = get_numerical_columns()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_cols = scaler.fit_transform(df[cols_to_scale])\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[cols_to_scale] = scaled_cols\n",
    "    return df_scaled\n",
    "\n",
    "def get_correlation_matrix(main_df, threshold=0.7):\n",
    "    corr_matrix = main_df.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    drop_cols = [col for col in upper_tri.columns if any(upper_tri[col] > threshold)]\n",
    "    uncorrelated_features = main_df.drop(columns=drop_cols)\n",
    "    print(\"Suggested columns to drop:\", drop_cols)\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "    return uncorrelated_features, drop_cols\n",
    "\n",
    "def create_groups(main_df, group_col):\n",
    "\n",
    "    %%time\n",
    "\n",
    "    # check which distances should be used for the column types\n",
    "    param_grid = {\n",
    "        'min_cluster_size': [150, 200],\n",
    "        'min_samples': [8, 10],\n",
    "        'metric': ['euclidean', 'manhattan', 'jaccard']\n",
    "    }\n",
    "\n",
    "    hdbscan_model = hdbscan.HDBSCAN()\n",
    "\n",
    "    def custom_silhouette_score(estimator, X):\n",
    "        labels = estimator.labels_\n",
    "        if len(set(labels)) <= 1:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return silhouette_score(X, labels)\n",
    "\n",
    "    grid_search = GridSearchCV(hdbscan_model, param_grid, cv=5, scoring=custom_silhouette_score)\n",
    "    grid_search.fit(cluster_df)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    best_hdbscan_model = hdbscan.HDBSCAN(**best_params)\n",
    "    best_hdbscan_model.fit(cluster_df)\n",
    "\n",
    "    labels = best_hdbscan_model.labels_\n",
    "    n_clusters= len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f'Number of clusters: {n_clusters}')\n",
    "    print(f'Silhouette score: {silhouette_score(cluster_df, labels)}')\n",
    "\n",
    "    return pd.Categorical(labels)\n",
    "\n",
    "def downsample(df, features, ratio=0.05):\n",
    "\n",
    "    target_col = get_target_column()\n",
    "\n",
    "    # Separate the majority and minority classes\n",
    "    majority = df[df[target_col] == 0]\n",
    "    minority = df[df[target_col] == 1]\n",
    "\n",
    "    # Undersample the majority class\n",
    "    n_minority = len(minority)\n",
    "    n_majority = int(n_minority * ratio)\n",
    "    majority_downsampled = resample(majority, replace=False, n_samples=n_majority, random_state=42)\n",
    "\n",
    "    # Combine the minority and undersampled majority classes\n",
    "    df_downsampled = pd.concat([minority, majority_downsampled])\n",
    "    y_test = df_downsampled[target_col]\n",
    "    df_downsampled = df_downsampled[features]\n",
    "\n",
    "    return df_downsampled, y_test\n",
    "\n",
    "def oversample_SMOTE(df_train, features):\n",
    "    target_col = get_target_column()\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train[target_col]\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "def oversample_ADASYN(df_train, features):\n",
    "    target_col = get_target_column()\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train[target_col]\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "def split_features_target(df, features):\n",
    "    target_col = get_target_column()\n",
    "    cols = df.columns.tolist()\n",
    "    pattern = re.compile(r\"_train$|_test$|_valid$\")\n",
    "    new_cols = [re.sub(pattern, \"\", col) for col in cols]\n",
    "    df.rename(columns=dict(zip(cols, new_cols)), inplace=True)\n",
    "    X = df[features]\n",
    "    Y = df[target_col]\n",
    "    return X, Y\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(f'Started Preprocess step at {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "    main_df = get_dataset()\n",
    "    target_col = get_target_column()\n",
    "    features = get_feature_columns()\n",
    "\n",
    "    # Split dataset into training, validation, and test sets\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Splitting dataset')\n",
    "    df_train, df_valid, df_test = split_data(main_df, test_size=0.50, validation_size=0.25, random_state=42)\n",
    "\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Starting feature processing')\n",
    "    steps = {'train': df_train, \n",
    "             'test': df_valid, \n",
    "             'valid': df_test}\n",
    "    for step, step_df in steps.items():\n",
    "        print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Pre processing {step} set')\n",
    "        missing_df = handle_missing_values(step_df)\n",
    "        date_df = extract_date_info(missing_df, ['fecha'])\n",
    "        encoded_df = transform_categorical(date_df, get_categorical_columns())\n",
    "        scaled_df = scale_numerical(encoded_df,get_numerical_columns())\n",
    "        df_final = scaled_df\n",
    "        steps[step] = df_final.rename(columns={col: col + \"_\" + step for col in df_final.columns})    \n",
    "\n",
    "    print(f'Train sets with {len(df_train)} rows')\n",
    "    print(f'Test sets with {len(df_test)} rows')\n",
    "    print(f'Valid sets with {len(df_valid)} rows')\n",
    "\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Split features and target sets and downsample train set')\n",
    "    X_train, y_train = oversample_ADASYN(df_train, target_col, features)\n",
    "    # X_train, y_train = split_features_target(df_train, target_col, features)\n",
    "    X_test, y_test = split_features_target(df_test, target_col, features)\n",
    "    X_valid, y_valid = split_features_target(df_valid, target_col, features)\n",
    "    \n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Checking feature correlation')\n",
    "    X_train, X_test, X_valid = get_correlation_matrix(X_train, X_test, X_valid)\n",
    "    \n",
    "    # Save sets to csv\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Saving sets to csv')\n",
    "    print(f'Train sets with {len(X_train)} rows')\n",
    "    print(f'Test sets with {len(X_test)} rows')\n",
    "    print(f'Valid sets with {len(X_valid)} rows')\n",
    "\n",
    "    X_train.to_csv(f'data/train.csv', index=False, header=True, encoding='utf-8')\n",
    "    X_test.to_csv(f'data/test.csv', index=False, header=True, encoding='utf-8')\n",
    "    X_valid.to_csv(f'data/valid.csv', index=False, header=True, encoding='utf-8')\n",
    "    y_train.to_csv(f'data/y_train.csv', index=False, header=True, encoding='utf-8')\n",
    "    y_test.to_csv(f'data/y_test.csv', index=False, header=True, encoding='utf-8')\n",
    "    y_valid.to_csv(f'data/y_valid.csv', index=False, header=True, encoding='utf-8')\n",
    "\n",
    "    print(f'Finished Preprocess step at {datetime.now().strftime(\"%H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
