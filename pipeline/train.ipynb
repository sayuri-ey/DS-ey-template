{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom modules\n",
    "from configs.custom_config import get_timezone, get_target_column, get_feature_columns, get_categorical_columns, get_numerical_columns\n",
    "# external modules\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn import linear_model, tree, ensemble, neighbors\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment and define functions\n",
    "\n",
    "# set timezone\n",
    "get_timezone()\n",
    "\n",
    "def get_sets(set):\n",
    "    X = pd.read_csv(f'data/{set}.csv')\n",
    "    y = pd.read_csv(f'data/y_{set}.csv')\n",
    "    return X, y\n",
    "\n",
    "def train_model(X_train, y_train, models):\n",
    "    trained_models = {}\n",
    "    for model_key, model in models.items():\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "        # plot_learning_curve(model, X_train, y_train)\n",
    "        \n",
    "        # Add the trained model object to the dictionary of trained models\n",
    "        trained_models[model_key] = model\n",
    "        \n",
    "    return trained_models\n",
    "\n",
    "def plot_precision_recall_curve(y_test, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    labels = ['Not Fraud', 'Fraud']\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           xlabel='Predicted label',\n",
    "           ylabel='True label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            text = f'{cm[i, j]} ({cm[i, j]/np.sum(cm)*100:.1f}%)'\n",
    "            ax.text(j, i, text,\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def plot_roc_curve(y_test, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_models(trained_models, X_test, y_test):\n",
    "    results = {}\n",
    "    y_preds = {}\n",
    "    y_test = y_test['fraude'].to_numpy()\n",
    "    for model_key, model in trained_models.items():\n",
    "        print(f'Evaluating {model_key} model')\n",
    "        print(model)\n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate precision, recall, and F1-score for the model\n",
    "        precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "        plot_precision_recall_curve(y_test, y_pred)\n",
    "        plot_roc_curve(y_test, y_pred)\n",
    "        plot_confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Add the evaluation results to the dictionary\n",
    "        results[model_key] = {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "        y_preds[f'{model_key}_y_pred'] = y_pred\n",
    "        print(f'Metrics for {model_key}: {results[model_key]}')\n",
    "        \n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df.index.name = 'model'\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, y_preds\n",
    "\n",
    "def plot_bar_chart(metrics_df):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = metrics_df.plot(kind='bar')\n",
    "    ax.set_xticklabels(metrics_df.index, rotation=0)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Metric Score')\n",
    "    plt.title('Comparison of Model Performance')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_box_plot(metrics_df):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = metrics_df.plot(kind='box')\n",
    "    ax.set_xticklabels(metrics_df.columns, rotation=0)\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Distribution of Model Performance')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model_metrics(models, X_test, y_test):\n",
    "    # create a grid of subplots\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=len(models), figsize=(15, 10))\n",
    "\n",
    "    # plot the performance metrics for each model\n",
    "    for i, model in enumerate(models):\n",
    "        # plot the precision-recall curve\n",
    "        plot_precision_recall_curve(model, X_test, y_test)\n",
    "        axes[0, i].set_title(f\"Model {i+1} PR Curve\")\n",
    "\n",
    "        # plot the ROC curve\n",
    "        plot_roc_curve(model, X_test, y_test)\n",
    "        axes[1, i].set_title(f\"Model {i+1} ROC Curve\")\n",
    "\n",
    "        # plot the confusion matrix\n",
    "        cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "        axes[2, i].imshow(cm, cmap=plt.cm.Blues)\n",
    "        axes[2, i].set_xticks([0, 1])\n",
    "        axes[2, i].set_yticks([0, 1])\n",
    "        axes[2, i].set_xticklabels([\"Negative\", \"Positive\"])\n",
    "        axes[2, i].set_yticklabels([\"Negative\", \"Positive\"])\n",
    "        axes[2, i].set_title(f\"Model {i+1} Confusion Matrix\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(f'Started Train step at {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "    # Read train set\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Read train set')\n",
    "    X_train, y_train = get_sets('train')\n",
    "    \n",
    "    #Train models\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Train dataset')\n",
    "    \n",
    "    # Models to test\n",
    "\n",
    "    models = {\n",
    "        \"logistic_regression\": linear_model.LogisticRegression(),\n",
    "        \"decision_trees\": tree.DecisionTreeClassifier(),\n",
    "        \"random_forest\": ensemble.RandomForestClassifier(),\n",
    "        \"knn\": neighbors.KNeighborsClassifier()\n",
    "    }\n",
    "    \n",
    "    trained_models = train_model(X_train, y_train, models)\n",
    "\n",
    "    # Get metrics, compare and choose\n",
    "    X_test, y_test = get_sets('test')\n",
    "    \n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Getting metrics')\n",
    "    metrics_df, y_preds = evaluate_models(trained_models, X_test, y_test)    \n",
    "    plot_bar_chart(metrics_df)\n",
    "    plot_box_plot(metrics_df)\n",
    "    # plot_model_metrics(models, X_test, y_test)\n",
    "    \n",
    "    print(y_preds)\n",
    "    \n",
    "    # Plots for chosen model\n",
    "    chosen_model = 'random_forest'\n",
    "    y_pred = y_preds[f'{chosen_model}_y_pred']\n",
    "    plot_precision_recall_curve(y_test, y_pred)\n",
    "    plot_roc_curve(y_test, y_pred)\n",
    "    plot_confusion_matrix(y_test, y_pred)    \n",
    "    # Save model\n",
    "    print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Saving model')\n",
    "    best_model = trained_models[chosen_model]\n",
    "    joblib.dump(best_model, 'model/model.pkl')\n",
    "\n",
    "    print(f'Finished Train step at {datetime.now().strftime(\"%H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
